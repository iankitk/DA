{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REGRESSORS_ASSEMBLE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUIbX3q3RauO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install h2o\n",
        "! pip install catboost\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.naive_bayes import GaussianNB #naiveayes can be used for regresio as well https://www.researchgate.net/publication/2360319_Naive_Bayes_for_Regression\n",
        "from xgboost import XGBRegressor,XGBRFRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn import tree\n",
        "\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from catboost import CatBoostRegressor #ModuleNotFoundError: No module named 'catboost'\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import cv2\n",
        "import h2o\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# import warnings to remove any type of future warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "data = pd.read_csv(\".csv\") #read your csv file here\n",
        "\n",
        "from google.colab import drive #importing from google drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data= pd.read_csv('/content/gdrive/My Drive/DA_GC_2020/train (1).csv')#,\n",
        "                  #  index_col=[0], parse_dates=[0]) # We set the index column and know it has dates\n",
        "data.head() \n",
        "\n",
        "data['Dates'] = pd.to_datetime(data['timestamp']).dt.date\n",
        "data['Time'] = pd.to_datetime(data['timestamp']).dt.time\n",
        "# data['timestamp'].dtypes\n",
        "data.head()\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "le=LabelEncoder()\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import train_test_split  \n",
        "for n in range(2):\n",
        "\tx_train, Xi_test, y_train, yi_test = train_test_split(X, y, test_size=0.8, random_state=60)  \n",
        "\tif cv2.waitKey(1) == ord('q' or 'Q'): break \n",
        "\n",
        "# Linear Regressor\n",
        "LinearRegressor = LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
        "\n",
        "# Polynomial Regressor\n",
        "PolyRegressor = PolynomialFeatures(degree=2, include_bias=True, interaction_only=False,\n",
        "                   order='C')\n",
        "\n",
        "#SVR Regressor \n",
        " RegressorSVM =SVR (C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
        "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
        "\n",
        "\n",
        " #RandomForesrtRegressor\n",
        "regressor = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
        "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "                      max_samples=None, min_impurity_decrease=0.0,\n",
        "                      min_impurity_split=None, min_samples_leaf=1,\n",
        "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
        "                      random_state=0, verbose=0, warm_start=False)\n",
        "\n",
        "\n",
        "\t#Logistic Regression \n",
        "RegressorLR = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
        "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
        "                   warm_start=False)\n",
        "\n",
        "\t#KNN regressor\n",
        "\tRegressorKNN =KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
        "                    weights='uniform')\n",
        " \n",
        "\n",
        "lg = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
        "              importance_type='split', learning_rate=0.1, max_depth=-1,\n",
        "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
        "              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
        "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False,\n",
        "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
        "\n",
        "#XGBoost regressor\n",
        "RegressorXGB =XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
        "             max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
        "             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
        "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "             silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "# XGBRFRegressor\n",
        "RegressorXGBRF=XGBRFRegressor(base_score=0.5, colsample_bylevel=1, colsample_bynode=0.8,\n",
        "               colsample_bytree=1, gamma=0, learning_rate=1, max_delta_step=0,\n",
        "               max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
        "               n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
        "               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "               silent=None, subsample=0.8, verbosity=1)\n",
        "\n",
        "#Bagging Regressor\n",
        "\tregressorBG =BaggingRegressor(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
        "                 max_features=1.0, max_samples=1.0, n_estimators=10,\n",
        "                 n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
        "                 warm_start=False) \n",
        " \n",
        " #Gradient Boosting Regressor\n",
        "\tregressorGB=  GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
        "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
        "                          max_features=None, max_leaf_nodes=None,\n",
        "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                          min_samples_leaf=1, min_samples_split=2,\n",
        "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "                          n_iter_no_change=None, presort='deprecated',\n",
        "                          random_state=None, subsample=1.0, tol=0.0001,\n",
        "                          validation_fraction=0.1, verbose=0, warm_start=False).fit(x_train, y_train)\n",
        "\n",
        "#Adaboost regressor\n",
        "regressorAB =AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear',\n",
        "                  n_estimators=50, random_state=None)\n",
        "\n",
        "\t#Catboost Regressor\n",
        "\n",
        "classifierCB=CatBoostRegressor(iterations=20, depth=5, learning_rate=0.1, loss_function='RMSE',silent=True)\n",
        "\tclassifierCB.fit(x_train, y_train,eval_set=(Xi_test, yi_test))\n",
        "\tpred = classifierCB.predict(X_ul)\n",
        "\tth=0.5\n",
        "\t#this is done as catboost provides answer in fractions like 1.02,0.98,etc\n",
        "\tpred[pred>th]=1\n",
        "\tpred[pred<=th]=0\n",
        "#couldn't get params see this documetation https://catboost.ai/docs/concepts/python-reference_catboost.html\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}